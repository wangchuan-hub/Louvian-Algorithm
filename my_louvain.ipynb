{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import copy\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_louvain:\n",
    "    def __init__(self, k):  # k:节点个数\n",
    "        self.k = k\n",
    "        \n",
    "    def get_data(self):\n",
    "        # np.random.seed(10)\n",
    "        data = np.zeros((self.k, self.k))\n",
    "        for i in range(len(data)):\n",
    "            for j in range(len(data)):\n",
    "                data[i,j] = np.random.choice([0,1], p=[0.9, 0.1])\n",
    "        data = data + np.multiply(data.T, data.T > data) - np.multiply(data, data.T > data)\n",
    "        for i in range(len(data)):\n",
    "            data[i, i] = 0\n",
    "        data = pd.DataFrame(data)\n",
    "        return data\n",
    "\n",
    "    # def get_data(self):\n",
    "    #     data = np.array([[0,1,1,1,0,0,0,0,0,0],\n",
    "    #                     [1,0,1,0,0,0,0,0,0,0],\n",
    "    #                     [1,1,0,1,0,0,0,0,0,0],\n",
    "    #                     [1,0,1,0,1,1,1,0,0,0],\n",
    "    #                     [0,0,0,1,0,1,0,0,0,0],\n",
    "    #                     [0,0,0,1,1,0,1,0,0,0],\n",
    "    #                     [0,0,0,1,0,1,0,0,0,0],\n",
    "    #                     [0,0,0,0,0,0,0,0,1,1],\n",
    "    #                     [0,0,0,0,0,0,0,1,0,1],\n",
    "    #                     [0,0,0,0,0,0,0,1,1,0]])\n",
    "    #     data = pd.DataFrame(data)\n",
    "    #     return data\n",
    "    \n",
    "    def Q_delta_i(self, sum_in, sum_tot, ki, ki_in, m, direction):\n",
    "        Q_before_i = (sum_in / 2 / m - (sum_tot / 2 / m) ** 2) + (0 - (ki / 2 / m) ** 2)\n",
    "        Q_after_i = (sum_in + ki_in) / 2 / m - ((sum_tot + ki) / 2 / m) ** 2\n",
    "\n",
    "        if direction == \"di\":\n",
    "            Q_delta_i = Q_before_i - Q_after_i\n",
    "        if direction == \"ic\":\n",
    "            Q_delta_i = Q_after_i - Q_before_i\n",
    "        return Q_delta_i\n",
    "\n",
    "    def Q_delta_dic(self, Q_delta_ic, Q_delta_di):\n",
    "        Q_delta = Q_delta_ic + Q_delta_di\n",
    "        return Q_delta\n",
    "\n",
    "\n",
    "    def cal_Q_delta(self, data, community_D, community_C): # 从D社区中拿出节点，放入C社区中。community_D:list community_C:list\n",
    "        \"\"\" \n",
    "        将 D 社区中的节点提取出来 放入 C 社区中, 需要分别计算将 node 从 D 社区 提取出来的Q_delta_di 和 将 node 放入 C社区的 Q_delta_ic\n",
    "        并且计算两个Q_delta 所用的 sum_in, sum_tot, ki, ki_in 是根据 node 的不同而改变的。具体而言: 把 node 从 D 社区提取出来时构造一个剔除 node 的 temp_list,\n",
    "        计算 node 对 temp_list 的四个统计数据, 把 node 放入 C 社区时计算 node 对 C 社区内所有节点的四个统计数据. 最终构造的字典 node 为 key Q_delta 为 value\n",
    "\n",
    "        注: 这里 community_D 和 community_C 是单个社区不同于下面的社区群组 community_Ds community_Cs\n",
    "        \"\"\"\n",
    "        m = data.sum().sum() / 2\n",
    "        Q_delta_dict = {}\n",
    "        for i in range(len(community_D)):\n",
    "            # 这里计算 将 i 节点 从 D 社区中拿出来的 一系列描述性统计数据以及 Q_delata_di\n",
    "            node_i = community_D[i]\n",
    "            temp_list = community_D[:i] + community_D[i+1:]    # 这个temp存储的是除了 i 节点外剩下的社区\n",
    "            sum_in_di = data.loc[temp_list, temp_list].sum().sum()\n",
    "            sum_tot_di = data.loc[temp_list,:].sum().sum()\n",
    "            ki = data.loc[node_i,:].sum()\n",
    "            ki_in_di = np.sum(np.sum(data.loc[node_i, temp_list])) * 2\n",
    "            Q_delta_di = self.Q_delta_i(sum_in_di, sum_tot_di, ki, ki_in_di, m, \"di\")\n",
    "\n",
    "            # 这里开始计算将 i 节点 放入 C 社区中的 一系列描述性统计数据以及Q_delta_ic\n",
    "            sum_in_ic = data.loc[community_C, community_C].sum().sum()\n",
    "            sum_tot_ic = data.loc[community_C, community_C + [node_i]].sum().sum()\n",
    "            ki_in_ic = data.loc[community_C, node_i].sum()\n",
    "            Q_delta_ic = self.Q_delta_i(sum_in_ic, sum_tot_ic, ki, ki_in_ic, m, \"ic\")\n",
    "            \n",
    "            # Q_delta_dic\n",
    "            Q_delta = self.Q_delta_dic(Q_delta_ic, Q_delta_di)\n",
    "            Q_delta_dict[node_i] = Q_delta\n",
    "        # print(Q_delta_dict)\n",
    "        return Q_delta_dict\n",
    "\n",
    "  \n",
    "\n",
    "    def phase_one(self,data, community_Ds, community_Cs):  \n",
    "        \"\"\"\n",
    "        community_Ds存放所有D社区, community_Cs存放所有C社区, 对于一个 commmunity_D 都计算将 community_D 中的每一个节点加入一个community_C中的Q_delta_dict\n",
    "        并且以该 community_C的索引为 key Q_delta_dict 为 value构造一个嵌套字典,转化成 dataframe index 为一个 community_D中的每一个节点, collumns 为community_Cs\n",
    "        中的每一个 community_C 根据 dataframe 方便的找出具体是 将 node 加入哪一个 community_C 中获得的增益 Q_delta 最大\n",
    "        对于该增益最大的 community_C 将 node 加入其中 \n",
    "        \"\"\"\n",
    "        mod_inc = False\n",
    "        while True:\n",
    "            can_stop = True\n",
    "            for community_D in community_Ds:\n",
    "                # 根据计算的 Q_delta 构造一个 dataframe\n",
    "                Q_delta_dicts = {}\n",
    "                for i in range(len(community_Cs)):\n",
    "                    if community_D == community_Cs[i]:\n",
    "                        pass\n",
    "                    else:\n",
    "                        Q_delta_dict = self.cal_Q_delta(data, community_D, community_Cs[i])\n",
    "                        Q_delta_dicts[i] = Q_delta_dict\n",
    "                    # print(Q_delta_dict)\n",
    "                df = pd.DataFrame(Q_delta_dicts, columns=Q_delta_dicts.keys())    # index 为一个 community_D中的每一个节点, collumns 为community_Cs中的每一个 community_C\n",
    "                # print(df)\n",
    "                # 根据构建的 dataframe 确定将 D 社区中的节点加入 具体某一个C中,即放入增益最大的社区C中\n",
    "                for i in reversed(range(len(df.index))):\n",
    "                    node = df.index[i]\n",
    "                    row = df.loc[node,:]\n",
    "                    Q = row.max()\n",
    "                    # print(\"Q\",Q)\n",
    "                    community = row[row==row.max()].index\n",
    "                    # community = sorted(row[row == row.max()].index)[0]\n",
    "                    community = np.random.choice(community)  # 当有多个社区的增益相同时, 随机挑选一个\n",
    "                    \n",
    "                    if Q > 0:\n",
    "                        for community_C in community_Cs:\n",
    "                            if node in community_C:\n",
    "                                community_C.remove(node)            \n",
    "                        community_Cs[community].append(node)\n",
    "                        community_Ds = community_Cs\n",
    "                        can_stop = False\n",
    "                        mod_inc = True\n",
    "                    # print(mod_inc)\n",
    "\n",
    "            print(community_Cs)\n",
    "            if can_stop:\n",
    "                break\n",
    "        # print(mod_inc)               \n",
    "            # 从 community_Cs 中删除节点时有可能会产生\"空社区\"(即该社区中的所有节点都转移到了其他社区,变成了一个空社区), 删除\"空社区\",得到phase_one的社区\n",
    "        while [] in community_Cs:\n",
    "            community_Cs.remove([])\n",
    "        return community_Cs, mod_inc\n",
    "    \n",
    "    def execute_phase_one(self, data, community_Cs):\n",
    "        community_Ds = community_Cs\n",
    "        community_Cs, mod_inc = self.phase_one(data, community_Ds, community_Cs)\n",
    "        # print(community_Cs)\n",
    "        # print(mod_inc)\n",
    "        return community_Cs, mod_inc\n",
    "\n",
    "\n",
    "    def phase_two(self, data, community_Cs):\n",
    "        large_node = {}\n",
    "        for i in range(len(community_Cs)):\n",
    "            # community_name = np.sum(community_Cs[i])\n",
    "            community_name = i\n",
    "            # print(community_name)\n",
    "            large_node[community_name] = community_Cs[i]\n",
    "        new_data = pd.DataFrame(index = large_node.keys(), columns = large_node.keys())\n",
    "        for i in new_data.index:\n",
    "            for j in new_data.columns:\n",
    "                new_data.loc[i,j] = np.sum(np.sum(data.loc[large_node[i], large_node[j]]))\n",
    "        return new_data, large_node\n",
    "\n",
    "    def execute(self):\n",
    "        iter_times = 0\n",
    "        data = self.get_data()\n",
    "        community_Cs = [[i] for i in range(len(data.columns))]\n",
    "        community_Ds = copy.deepcopy(community_Cs)\n",
    "        while True:\n",
    "            iter_times += 1\n",
    "            community_Cs, mod_inc = self.phase_one(data, community_Ds, community_Cs)\n",
    "            # print(community_Cs)\n",
    "            if mod_inc:\n",
    "                data, large_node = self.phase_two(data, community_Cs)\n",
    "                print(\"iter_times:\", iter_times, large_node)\n",
    "                community_Cs = [[i] for i in large_node.keys()]\n",
    "                community_Ds = copy.deepcopy(community_Cs)\n",
    "                # last_community = \n",
    "            else:\n",
    "                break\n",
    "\n",
    "            \n",
    "    # def execute_phase(self):\n",
    "    #     data = self.get_data()\n",
    "    #     community_Cs = [[i] for i in data.columns]\n",
    "    #     random.shuffle(community_Cs)\n",
    "\n",
    "\n",
    "        # community_Ds = copy.deepcopy(community_Cs)\n",
    "        # community = self.phase_one(data, community_Ds, community_Cs)\n",
    "        # # print(community)\n",
    "        # return community_Cs\n",
    "\n",
    "    # def execute_all_phase(self):\n",
    "    #     # np.random.seed(10)\n",
    "    #     data = ml.get_data()\n",
    "    #     community_Cs = [[i] for i in data.columns]\n",
    "    #     random.shuffle(community_Cs)\n",
    "    #     for i in range(self.iterations):\n",
    "    #         community_Cs = self.execute_phase_one(data, community_Cs)\n",
    "    #         print(\"第{}次,phase_one\".format(i), community_Cs)\n",
    "    #         community_Cs = self.phase_two(data, community_Cs)\n",
    "    #         print(\"第{}次,phase_two\".format(i), community_Cs)\n",
    "    #     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], [], [], [5, 47], [7, 14], [2, 21], [9, 36], [], [4, 32, 42], [], [12, 38, 40], [0, 11, 13, 19, 22, 27, 30, 37, 41, 43], [], [], [], [], [], [16, 29], [3, 24], [], [], [], [], [18], [], [15, 23, 33], [], [], [], [20], [1], [6, 8, 26, 28, 31, 39, 44], [25], [], [], [35], [], [], [], [], [17], [10, 34, 45, 46, 48, 49]]\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [5, 47], [], [2, 21], [36], [], [32, 42], [], [12, 38, 40, 7], [0, 11, 13, 19, 22, 27, 30, 41, 43], [], [], [], [], [], [16, 29], [3, 24, 20, 35], [], [], [], [], [], [], [15, 23, 33, 4, 1], [], [], [], [], [], [6, 8, 26, 28, 31, 39, 44, 9, 18, 25], [], [], [], [], [], [], [], [], [], [10, 34, 45, 46, 48, 49, 14, 37, 17]]\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [5, 47], [], [2, 21, 36], [], [], [32, 42], [], [12, 38, 40, 7], [0, 11, 13, 19, 22, 27, 30, 41, 43], [], [], [], [], [], [16, 29], [3, 24, 20, 35], [], [], [], [], [], [], [15, 23, 33, 4, 1], [], [], [], [], [], [6, 8, 26, 28, 31, 39, 44, 9, 18, 25], [], [], [], [], [], [], [], [], [], [10, 34, 45, 46, 48, 49, 14, 37, 17]]\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [5, 47], [], [2, 21, 36], [], [], [32, 42], [], [12, 38, 40, 7], [0, 11, 13, 19, 22, 27, 30, 41, 43], [], [], [], [], [], [16, 29], [3, 24, 20, 35], [], [], [], [], [], [], [15, 23, 33, 4, 1], [], [], [], [], [], [6, 8, 26, 28, 31, 39, 44, 9, 18, 25], [], [], [], [], [], [], [], [], [], [10, 34, 45, 46, 48, 49, 14, 37, 17]]\n",
      "iter_times: 1 {0: [5, 47], 1: [2, 21, 36], 2: [32, 42], 3: [12, 38, 40, 7], 4: [0, 11, 13, 19, 22, 27, 30, 41, 43], 5: [16, 29], 6: [3, 24, 20, 35], 7: [15, 23, 33, 4, 1], 8: [6, 8, 26, 28, 31, 39, 44, 9, 18, 25], 9: [10, 34, 45, 46, 48, 49, 14, 37, 17]}\n",
      "[[], [], [1, 2, 9], [], [], [4], [5, 6], [0, 3, 7], [8], []]\n",
      "[[], [], [1, 2, 9], [], [], [4, 0], [5, 6], [3, 7], [8], []]\n",
      "[[], [], [1, 2, 9], [], [], [4, 0], [5, 6], [3, 7], [8], []]\n",
      "iter_times: 2 {0: [1, 2, 9], 1: [4, 0], 2: [5, 6], 3: [3, 7], 4: [8]}\n",
      "[[0], [1], [2], [3], [4]]\n"
     ]
    }
   ],
   "source": [
    "ml = My_louvain(k = 50)\n",
    "ml.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07efdcd4b820c98a756949507a4d29d7862823915ec7477944641bea022f4f62"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
